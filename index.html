<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tingle Li</title>
  
  <meta name="author" content="Tingle Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tingle Li</name>
		<br>
              </p>
              <p>
                I am a research assistant at <a href="https://iiis.tsinghua.edu.cn/en/">IIIS, Tsinghua University</a>, advised by Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a>. I am also lucky to work with Prof. <a href="http://andrewowens.com/">Andrew Owens</a> from <a href="https://umich.edu/">The University of Michigan</a>.
      		    </p>
      		    <p>
                Previously, I received my B.E. from <a href="http://en.tiangong.edu.cn/">Tiangong University</a>, and was a research intern at <a href="https://dukekunshan.edu.cn/en">Duke University (China campus)</a>, advised by Prof. <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>.
              </p>
              <p>
                I study the perceptual insights brought by naturally paired data (audio, images, video). These may include speech processing and audio-visual learning.
              </p>
              <p>
              
               
              </p>
              <p style="text-align:center">
                <a href="mailto:tingle.li@outlook.com">Email</a> &nbsp/&nbsp
                <a href="CV_Tingle_Li.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=UGpC1zgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Tinglok"> GitHub </a>
    
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                2021
                  <ul>
                      <li>One paper was accepted by <a href="https://nips.cc/Conferences/2021/">NeurIPS 2021</a>, stay tuned!</li>
                      <li>One paper was accepted by <a href="https://www.interspeech2021.org/">Interspeech 2021</a> with <a href="https://www.interspeech2021.org/student-information/travel-grants">ISCA Travel Grant Award</a>, please check it out!</li>
                  </ul>
                2020
                  <ul>
                      <li>Our DKU systems were ranked 1<sup>st</sup> in SID and 3<sup>rd</sup> in SAD among all the participants in <a href="https://fearless-steps.github.io/ChallengePhase2/index.html">Fearless Steps Challenge-2</a>.</li>

                  </ul>

              </p>
            </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Publications</heading>
        

          </td>
            </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
		        	<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/LFT-teaser.jpg" alt="lft" width="160" height="90">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://drive.google.com/file/d/1KnQm_H1ZKJNGt3YfvdzKd8KCosJnaRoT/view?usp=sharing">
                    <papertitle> Listen for Texture: Learning Visual Style from Audio-Visual Associations </papertitle>
                  </a>
                  <br>
              			<b>Tingle Li</b>, <a href="https://www.linkedin.com/in/yichen-liu-751804176/">Yichen Liu</a>, <a href="https://andrewowens.com/">Andrew Owens</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                  <br>
                  <em>In submission to CVPR</em>, 2022 &nbsp 
                  <br>
                  <a href="https://drive.google.com/file/d/1KnQm_H1ZKJNGt3YfvdzKd8KCosJnaRoT/view?usp=sharing">PDF</a> / <a href="https://drive.google.com/file/d/1AGipfl4CfD7uvPGGJ-ZBWfqaIRS6V2jA/view?usp=sharing">Video</a>
                  <p></p>
                  <p>A GAN-based method to link audio signals to the style and material properties of visual scenes. </p>
                </td>
              </tr>
		
        		  <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/UMT-teaser.jpg" alt="umt" width="160" height="90">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openreview.net/pdf?id=1eGFH6yYAJn">
                    <papertitle> Modality Laziness: Everybody's Business is Nobody's Business </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=VoF-UAEAAAAJ&hl=en">Chenzhuang Du</a>, <a href="https://www.tengjiaye.com/">Jiaye Teng</a>, <b>Tingle Li</b>, <a href="https://www.linkedin.com/in/yichen-liu-751804176/">Yichen Liu</a>, <a href="https://people.csail.mit.edu/yuewang/">Yue Wang</a>, <a href="http://people.iiis.tsinghua.edu.cn/~yuanyang/en.html">Yang Yuan</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                  <br>
                  <em>In submission to ICLR</em>, 2022 &nbsp 
                  <br>
                  <a href="https://openreview.net/pdf?id=1eGFH6yYAJn">PDF</a>
                  <p></p>
                  <p>With multi-modal data as inputs, the encoders from naive fusion training will suffer from learning insufficient representations of each modality. </p>
                </td>
              </tr>
		
		
		        <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/AVD-teaser.png" alt="avd" width="160" height="95">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://papers.nips.cc/paper/2021/file/8a9c8ac001d3ef9e4ce39b1177295e03-Paper.pdf">
                    <papertitle> Neural Dubber: Dubbing for Videos According to Scripts </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=4LzKZggAAAAJ&hl=en">Chenxu Hu</a>, <a href="https://scholar.google.com/citations?user=PMH1tnEAAAAJ&hl=en">Qiao Tian</a>, <b>Tingle Li</b>, Yuping Wang, <a href="https://www.linkedin.com/in/yuxuan-wang-85b5704">Yuxuan Wang</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                  <br>
                  <em>NeurIPS</em>, 2021 &nbsp 
                  <br>
                  <a href="https://papers.nips.cc/paper/2021/file/8a9c8ac001d3ef9e4ce39b1177295e03-Paper.pdf">PDF</a> / <a href="https://tsinghua-mars-lab.github.io/NeuralDubber/">Project Page</a> / <a href="https://slator.com/neural-dubber-tiktok-company-bytedance-explores-automated-dubbing/">Press</a>
                  <p></p>
                  <p>Automatic video dubbing driven by a neural network. </p>
                </td>
              </tr>
		        <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/CVC-teaser.jpg" alt="CATE" width="160" height="95">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/li21d_interspeech.pdf">
                    <papertitle> CVC: Contrastive Learning for Non-parallel Voice Conversion </papertitle>
                  </a>
                  <br>
                  Chen Sun, <b>Arsha Nagrani</b>, Yonglong Tian, Cordelia Schmid
                  <br>
                  <em>Interspeech</em>, 2021 <font color="red"><strong>(ISCA Student Travel Grant Award)</strong></font>&nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/li21d_interspeech.pdf">PDF</a> / <a href="https://tinglok.netlify.app/files/cvc"> Project Page</a> / <a href="https://github.com/Tinglok/CVC"> Code </a> 
                  <p></p>
                  <p> One-way GAN training for non-parallel voice conversion. </p>
                </td>
              </tr>
		
        <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Sams-teaser.jpg" alt="sams" width="160" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/1909.05746.pdf">
                    <papertitle> Sams-Net: A Sliced Attention-based Neural Network for Music Source Separation </papertitle>
                  </a>
                  <br>
                  <b>Tingle Li</b>, Jiawei Chen, Haowen Hou, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>ISCSLP</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/1909.05746.pdf">arXiv</a> / <a href="https://tinglok.netlify.app/files/samsnet/">Project Page</a> 
                  <p></p>
                  <p> The scope of attention is narrowed down to the intra-chunk musical features that are most likely to affect each other. </p>
                </td>
              </tr>
		
	<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/FSC-teaser.jpg" alt="fsc" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/lin20e_interspeech.pdf">
                    <papertitle> The DKU Speech Activity Detection and Speaker Identification Systems for Fearless Steps Challenge Phase-02 </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=GM8wpNcAAAAJ&hl=en">Qingjian Lin</a>, <b>Tingle Li</b>, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>Interspeech</em>, 2020 &nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/lin20e_interspeech.pdf">PDF</a> / <a href="https://fearless-steps.github.io/ChallengePhase2/Final.html">Leaderboard</a>
                  <p></p>
                  <p>SoTA performance for speech activity detection and speaker identification. </p>
                </td>
              </tr>
		
	<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Atss-teaser.jpg" alt="atss" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/li20p_interspeech.pdf">
                    <papertitle> Atss-Net: Target Speaker Separation via Attention-based Neural Network </papertitle>
                  </a>
                  <br>
                  <b>Tingle Li</b>, <a href="https://scholar.google.com/citations?user=GM8wpNcAAAAJ&hl=en">Qingjian Lin</a>, Yuanyuan Bao, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>Interspeech</em>, 2020 &nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/li20p_interspeech.pdf">PDF</a> / <a href="https://tinglok.netlify.app/files/interspeech2020/">Project Page</a>
                  <p></p>
                  <p>Adapted Transformer to the speech separation for more efficient and generalizable performance. 
              </tr>
		
		
		<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/OML-teaser.jpg" alt="oml" width="160" height="120">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-speech.org/archive/pdfs/odyssey_2020/lin20_odyssey.pdf">
                    <papertitle> Optimal Mapping Loss: A Faster Loss for End-to-End Speaker Diarization </papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=GM8wpNcAAAAJ&hl=en">Qingjian Lin</a>, <b>Tingle Li</b>, Lin Yang, Junjie Wang, <a href="https://scholars.duke.edu/person/MingLi">Ming Li</a>
                  <br>
                  <em>Odyssey</em>, 2020 &nbsp 
                  <br>
                  <a href="https://www.isca-speech.org/archive/pdfs/odyssey_2020/lin20_odyssey.pdf">PDF</a>
                  <p></p>
                  <p>A new mapping loss based on Hungarian algorithm that reduces time complexity while maintaining performance for speaker diarization.
              </tr>
    
<!-- Default Statcounter code for Tingle's homepage https://tinglok.netlify.app/ -->
<script type="text/javascript">
var sc_project=12691142; 
var sc_invisible=1; 
var sc_security="b5064dc1"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12691142/0/b5064dc1/1/"
alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->




        
</body>
</html>
